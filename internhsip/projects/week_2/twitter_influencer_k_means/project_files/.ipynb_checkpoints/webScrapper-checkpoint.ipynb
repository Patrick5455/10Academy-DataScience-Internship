{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### functions for search with with bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile ../pyscrap_url.py\n",
    "\n",
    "def simple_get(url):\n",
    "    \"\"\"\n",
    "    Attempts to get the content at `url` by making an HTTP GET request.\n",
    "    If the content-type of response is some kind of HTML/XML, return the\n",
    "    text content, otherwise return None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with closing(get(url)) as resp:\n",
    "            #resp = get(url)\n",
    "            if is_good_response(resp):\n",
    "                return resp.content#.encode(BeautifulSoup.original_encoding)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        log_error('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    \"\"\"\n",
    "    Returns True if the response seems to be HTML, False otherwise.\n",
    "    \"\"\"\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200 \n",
    "            and content_type is not None \n",
    "            and content_type.find('html') > -1)\n",
    "\n",
    "\n",
    "def log_error(e):\n",
    "    \"\"\"\n",
    "    It is always a good idea to log errors. \n",
    "    This function just prints them, but you can\n",
    "    make it do anything.\n",
    "    \"\"\"\n",
    "    print(e)\n",
    "    \n",
    "def get_elements(url, tag='',search={}, fname=None):\n",
    "    \"\"\"\n",
    "    Downloads a page specified by the url parameter\n",
    "    and returns a list of strings, one per tag element\n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(url,str):\n",
    "        response = simple_get(url)\n",
    "    else:\n",
    "        #if already it is a loaded html page\n",
    "        response = url\n",
    "\n",
    "    if response is not None: \n",
    "        html = BeautifulSoup(response, 'html.parser')\n",
    "        \n",
    "        res = []\n",
    "        if tag:    \n",
    "            for li in html.select(tag):\n",
    "                for name in li.text.split('\\n'):\n",
    "                    if len(name) > 0:\n",
    "                        res.append(name.strip())\n",
    "                       \n",
    "                \n",
    "        if search:\n",
    "            soup = html            \n",
    "            \n",
    "            \n",
    "            r = ''\n",
    "            if 'find' in search.keys():\n",
    "                print('finding',search['find'])\n",
    "                soup = soup.find(**search['find'])\n",
    "                r = soup\n",
    "\n",
    "                \n",
    "            if 'find_all' in search.keys():\n",
    "                print('findaing all of',search['find_all'])\n",
    "                r = soup.find_all(**search['find_all'])\n",
    "   \n",
    "            if r:\n",
    "                for x in list(r):\n",
    "                    if len(x) > 0:\n",
    "                        res.extend(x)\n",
    "            \n",
    "        return res\n",
    "\n",
    "    # Raise an exception if we failed to get any data from the url\n",
    "    raise Exception('Error retrieving contents at {}'.format(url))    \n",
    "    \n",
    "    \n",
    "# if get_ipython().__class__.__name__ == '__main__':\n",
    "#     fire(get_tag_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Scrape https://www.socialbakers.com/statistics/twitter/profiles/nigeria/ for influential handles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# #PATH = os.path.join(\"C:\\\\\",\"Users\",\"frevert\",\"Documents\",\"py\")\n",
    "# def table_to_df(table):\n",
    "#     print(table)\n",
    "#     return pd.DataFrame([[td.text for td in row.findAll('td')] for row in table.findAll('tr')])\n",
    "\n",
    "# def next_page(soup):\n",
    "# \treturn \"http:\" + soup.find('a', attrs={'rel':'next'}).get('href')\n",
    "# res = pd.DataFrame()\n",
    "# url = \"https://www.socialbakers.com/statistics/twitter/profiles/nigeria\"\n",
    "\n",
    "# counter = 0\n",
    "# while True:\n",
    "# \tprint(counter)\n",
    "# \tpage = get(url)#, print(page)\n",
    "# \tsoup = BeautifulSoup(page.content, 'lxml')#,print(soup) \n",
    "# \ttable = soup.find(name='table', attrs={'class':'brand-table-list'}), print(type(table))\n",
    "# \t#res = res.append(table_to_df(table))\n",
    "# \t#res.to_csv('project_files/datasets/scraped_hanldes', index=None, sep=';', encoding='iso-8859-1')\n",
    "# \turl = next_page(soup)\n",
    "# \tcounter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages (func= get_elements, country ='nigeria', pages = 2):\n",
    "    influencers = [] \n",
    "    url = f'https://www.socialbakers.com/statistics/twitter/profiles/{country}'\n",
    "    #url = 'https://www.socialbakers.com/statistics/twitter/profiles/nigeria/media/daily-news'\n",
    "    while pages:\n",
    "        print(url)\n",
    "        names = func(url,search={'find':{'class_':'brand-table-list'}, 'find_all':{'class_':'name'}}, tag='span')#[4:14]\n",
    "        influencers.append(names)\n",
    "        page = get(url)\n",
    "        soup = BeautifulSoup(page.content, 'lxml')\n",
    "        url = \"https://www.socialbakers.com\" + soup.find('a', attrs={'rel':'next'}).get('href')\n",
    "        pages-=1\n",
    "    #print(influencers)\n",
    "    return influencers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NB:` The website serves only 10 names per page... hence to get the top 100, we have to loop 10 times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url ='https://www.socialbakers.com/statistics/twitter/profiles/nigeria/page-1-3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Firefox(executable_path=r\"/home/patrick/geckodriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = driver.find_elements_by_class_name('acc-placeholder-img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(@wizkidayo)'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table[18].find_element_by_tag_name('h2').text.split(' ')[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Show More Twitter Profiles in Nigeria'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "driver.find_elements_by_class_name('more-center-link')[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import logging\n",
    "import re\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.linkextractors.lxmlhtml import LxmlLinkExtractor\n",
    "from googlesearch import search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(tags, n, language):\n",
    "    urls = []\n",
    "    for tag in tags:\n",
    "        print('searching google... for '+tag)\n",
    "        tag_url = [url for url in \n",
    "               search(tag+' twitter', stop=n, lang=language, country='Nigeria')][:n]\n",
    "        urls.extend(tag_url)\n",
    "    print('done searching', '\\ncollecting tweets only')\n",
    "    for idx, i in enumerate(urls):\n",
    "        if 'hashtag' in i:\n",
    "            urls.pop(idx)\n",
    "    print('done') \n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gather Tweet Links (using hashtags & popular figures search) across the following doamins\n",
    "\n",
    "- a. Economy\n",
    "\n",
    "- b. Social values (sport, education, human rights, etc.)\n",
    "\n",
    "- c. Cultural (entertainment, fashion, art, etc)\n",
    "\n",
    "- d. Public health"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _______________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Economy hashtags/keyowrds\n",
    "\n",
    "     #CBN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "economic_tags = ['#CBN', '#nigerianbanks', 'money', 'capital']\n",
    "\n",
    "economic = get_urls( economic_tags, 5, 'en')\n",
    "economic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Social values (sport, education, human rights, etc.) hashtags\n",
    "\n",
    "        #FAAN, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_tags = ['#FAAN', '#AuditMoneyTrail', ' #NDDCProbe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social = get_urls(social_tags, 5, 'en')\n",
    "social"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cultural (entertainment, fashion, art, etc) hashtags\n",
    "\n",
    " #bbnajia2020, #MercyEkeBrands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cultural_tags = ['#LayconVerified', '#bbnajia2020', '#MercyEkeBrands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "culture = get_urls( cultural_tags, 5, 'en')\n",
    "culture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Public health hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publicHealth_tags = ['#covid19 nigeria ', '#corona nigeria ', '#coronavirus nigeria ', '#healthcare nigeria']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publicHealth = get_urls(publicHealth_tags, 5, 'en')\n",
    "publicHealth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx, i in enumerate(publicHealth):\n",
    "#     print(idx, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter_influencer_k_means",
   "language": "python",
   "name": "twitter_influencer_k_means"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
